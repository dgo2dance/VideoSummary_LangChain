{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTrybxlELBGJ",
        "outputId": "8adfeabf-fbe3-40af-d408-5872a707b78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.18)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.19)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSW9h9GmLMPr",
        "outputId": "51a436f3-b8ec-4ea3-9488-101fe2692025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a6uJYdPPDiH"
      },
      "source": [
        "sacremoses is a Python library built NLP tasks involving multiple languages.\n",
        "\n",
        "Text normalization: Cleans and prepares text for downstream NLP tasks by handling things like case normalization, punctuation, and diacritics.\n",
        "Tokenization: Splits text into discrete units for processing, such as splitting words or subwords.\n",
        "Truecasing: Converts text to its correct capitalization.\n",
        "\n",
        "Language detection: Identifies the language of a given text snippet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxqVf0KiLVRl"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "# adding progress bars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlKHRcU72v1H",
        "outputId": "36bec8d9-2e3d-406e-ca7c-d0e9f9c87e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.11.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.1)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV2r9UM2LXFo"
      },
      "outputs": [],
      "source": [
        "import asyncio # for callback only(Asynchronous programming)\n",
        "from langchain.callbacks import get_openai_callback #(interacting with OPENAI's API)\n",
        "from langchain.document_loaders import YoutubeLoader #(Loading content from YT Videos)\n",
        "from langchain import OpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain #(Text Summarization)\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter #(Text Splitting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhJ4xQGnrM1a",
        "outputId": "e1cf7d1f-f5b5-42b9-afc1-94077b169611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.2.2)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.2\n"
          ]
        }
      ],
      "source": [
        "pip install youtube-transcript-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxUXxSJILdVL",
        "outputId": "1a93529e-8dc6-4dcd-ebd6-07b4c53f998f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(page_content=\"in this step-by-step guide you'll learn how to use langen autogen retrieval augmented generation Rag and function calls to build a super AI chatbot for you guys watching over my shoulder every step of the way this video is going to be hugely valuable to anyone who is new to autogen Lang chin and function call and wants to very quickly wrap their head around artificial intelligence space how to build and explain the code you are going to see the end result which I was able to create the code and how you guys can take the exact same code and use for your personal project definitely stay tuned throughout the end of this video If you guys haven't followed me I highly recommend that you do so so you can stay up to date with the latest AI news lastly make sure you guys subscribe turn the notification Bell like this video and check out previous videos because there is a lot of content that you will definitely benefit from so that thought let's get right back into the video for those who may not be as familiar with with technical details let me provide a bit of background to make things easier to understand what is Lang chain Lang chain is an open-source library that provides developers with the tools to build LM applications powered by large language models llms such as open AI or hugging face this allows you to build Dynamic data responsive applications that harness the most recent breakthroughs in natural language processing what is autogen autogen is not just a tool it's the future of collaborative AI where multiple Agents come together to transform ideas into reality and where AI agents unite innovate and Elevate what is different between autogen and Lang chain put simply autogen and Lang chain are both Frameworks for developing llm driven applications however there are some key differences between the two autogen is a multi-agent framework while Lang chain is a single agent framework autogen is more focused on code generation while Lang chain is more focused on general purpose NLP tasks what is retrieval augmented generation retrieval augmented generation is an AI framework that retrieves data from external sources of knowledge to improve the quality of responses it ensures accuracy through techniques like vector similarity search and realtime updat to external data sets let's start coding we going start installing the necessary requirements assuming you have created a new python project and set up a virtual environment run the command then let's import the required dependencies we set up the list to autogen we create the config list as follows config list is a list containing configuration settings for the model you intend to use seed we set the seed to 42 for caching purposes with this configuration we are ready to use AI agents with autogen in this snippet we upload a PDF file and process it Pi pdf2 is used to read the PDF file text Splitter from L chain is used to split the text into chunks open AI embeddings is used for embedding the PDF file then f is used to create a vector store viice is used to convert the text chunks into Vector embeddings these vectors can then be used for various applications like similarity search Once the database has been created we can then query it we are going make use of the conversational retrieval chain from Lang chain to connect our similarity search to the prompts user input and the we call conversation buffer memory is a simple memory buffer that stores the history of the conversation Auden agents supports function calling for open AI models but we need to specify the function using the following snippet let's create an automated assistant agent named assistant with specific configurations the assistant is designed to interact with users and provide accurate responses this allows the assistant to read a PDF and generate llm an accurate answer user proxy agents are the types of AI agents that work on behalf of the user the implementation is typical but it includes a unique feature the function D map parameter this parameter is used to link the configuration for function calls with the actual functions themselves ensuring seamless integration and operation once the agents are set up the script starts a conversation between the user and the chatbot this is done by calling the initiate chat method on the user proxy object the initiate chat method requires two parameters the assistant instance which acts as the chatbot and a text message that outlines the task to be discussed the result looks like this in this video we explained how I use autogen Lang chain function call and retrieval augmented generation to create a super AI chatbot when these components are combined the responses will be significantly more powerful and versatile it would be capable of handling complex tasks more efficiently generating more relevant and context aware content and accessing a wider range of information to inform its responses I will leave all these links in the description below so that you can easily access them it's a great read and it'll give you a lot more understanding as to how they basically accomplish this so with that thought I genuinely hope you found it informative and valuable if you did please give it a thumbs up and consider subscribing for more content like this don't forget to click the notification Bell so you never miss an update from us if you have any questions or thoughts drop them in the comments below I always love hearing from you until next time stay curious and keep learning\", metadata={'source': '36fivbd5D0o'})]\n"
          ]
        }
      ],
      "source": [
        "# loader = YoutubeLoader(video_id=\"1aA1WGON49E\")\n",
        "loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=36fivbd5D0o\")\n",
        "#Creates a YoutubeLoader object to load the transcript of the specified video ID.\n",
        "result = loader.load()\n",
        "#Fetches the transcript from YouTube and stores it in the result variable.\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ2rLo_vLrck"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=20)\n",
        "#Creates a RecursiveCharacterTextSplitter object to split the transcript into chunks of 2000 characters with a 20-character overlap.\n",
        "out = text_splitter.split_documents(result)\n",
        "#Splits the transcript into chunks and stores them in the out list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99hjzSJhMDxB",
        "outputId": "a5938f30-7a2e-4d78-9468-312ece3622b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(page_content=\"in this step-by-step guide you'll learn how to use langen autogen retrieval augmented generation Rag and function calls to build a super AI chatbot for you guys watching over my shoulder every step of the way this video is going to be hugely valuable to anyone who is new to autogen Lang chin and function call and wants to very quickly wrap their head around artificial intelligence space how to build and explain the code you are going to see the end result which I was able to create the code and how you guys can take the exact same code and use for your personal project definitely stay tuned throughout the end of this video If you guys haven't followed me I highly recommend that you do so so you can stay up to date with the latest AI news lastly make sure you guys subscribe turn the notification Bell like this video and check out previous videos because there is a lot of content that you will definitely benefit from so that thought let's get right back into the video for those who may not be as familiar with with technical details let me provide a bit of background to make things easier to understand what is Lang chain Lang chain is an open-source library that provides developers with the tools to build LM applications powered by large language models llms such as open AI or hugging face this allows you to build Dynamic data responsive applications that harness the most recent breakthroughs in natural language processing what is autogen autogen is not just a tool it's the future of collaborative AI where multiple Agents come together to transform ideas into reality and where AI agents unite innovate and Elevate what is different between autogen and Lang chain put simply autogen and Lang chain are both Frameworks for developing llm driven applications however there are some key differences between the two autogen is a multi-agent framework while Lang chain is a single agent framework autogen is more focused on code generation while Lang chain is more focused on\", metadata={'source': '36fivbd5D0o'}), Document(page_content=\"is more focused on general purpose NLP tasks what is retrieval augmented generation retrieval augmented generation is an AI framework that retrieves data from external sources of knowledge to improve the quality of responses it ensures accuracy through techniques like vector similarity search and realtime updat to external data sets let's start coding we going start installing the necessary requirements assuming you have created a new python project and set up a virtual environment run the command then let's import the required dependencies we set up the list to autogen we create the config list as follows config list is a list containing configuration settings for the model you intend to use seed we set the seed to 42 for caching purposes with this configuration we are ready to use AI agents with autogen in this snippet we upload a PDF file and process it Pi pdf2 is used to read the PDF file text Splitter from L chain is used to split the text into chunks open AI embeddings is used for embedding the PDF file then f is used to create a vector store viice is used to convert the text chunks into Vector embeddings these vectors can then be used for various applications like similarity search Once the database has been created we can then query it we are going make use of the conversational retrieval chain from Lang chain to connect our similarity search to the prompts user input and the we call conversation buffer memory is a simple memory buffer that stores the history of the conversation Auden agents supports function calling for open AI models but we need to specify the function using the following snippet let's create an automated assistant agent named assistant with specific configurations the assistant is designed to interact with users and provide accurate responses this allows the assistant to read a PDF and generate llm an accurate answer user proxy agents are the types of AI agents that work on behalf of the user the implementation is typical but it includes\", metadata={'source': '36fivbd5D0o'}), Document(page_content=\"but it includes a unique feature the function D map parameter this parameter is used to link the configuration for function calls with the actual functions themselves ensuring seamless integration and operation once the agents are set up the script starts a conversation between the user and the chatbot this is done by calling the initiate chat method on the user proxy object the initiate chat method requires two parameters the assistant instance which acts as the chatbot and a text message that outlines the task to be discussed the result looks like this in this video we explained how I use autogen Lang chain function call and retrieval augmented generation to create a super AI chatbot when these components are combined the responses will be significantly more powerful and versatile it would be capable of handling complex tasks more efficiently generating more relevant and context aware content and accessing a wider range of information to inform its responses I will leave all these links in the description below so that you can easily access them it's a great read and it'll give you a lot more understanding as to how they basically accomplish this so with that thought I genuinely hope you found it informative and valuable if you did please give it a thumbs up and consider subscribing for more content like this don't forget to click the notification Bell so you never miss an update from us if you have any questions or thoughts drop them in the comments below I always love hearing from you until next time stay curious and keep learning\", metadata={'source': '36fivbd5D0o'})]\n"
          ]
        }
      ],
      "source": [
        "print(out)\n",
        "#Prints the list of text chunks to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Twu0qzWS06"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature =0,openai_api_key='sk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFZfc0dKMRcf",
        "outputId": "e34b959d-d312-4d8c-be57-78f44995a9b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
            "\n",
            "\n",
            "\"in this step-by-step guide you'll learn how to use langen autogen retrieval augmented generation Rag and function calls to build a super AI chatbot for you guys watching over my shoulder every step of the way this video is going to be hugely valuable to anyone who is new to autogen Lang chin and function call and wants to very quickly wrap their head around artificial intelligence space how to build and explain the code you are going to see the end result which I was able to create the code and how you guys can take the exact same code and use for your personal project definitely stay tuned throughout the end of this video If you guys haven't followed me I highly recommend that you do so so you can stay up to date with the latest AI news lastly make sure you guys subscribe turn the notification Bell like this video and check out previous videos because there is a lot of content that you will definitely benefit from so that thought let's get right back into the video for those who may not be as familiar with with technical details let me provide a bit of background to make things easier to understand what is Lang chain Lang chain is an open-source library that provides developers with the tools to build LM applications powered by large language models llms such as open AI or hugging face this allows you to build Dynamic data responsive applications that harness the most recent breakthroughs in natural language processing what is autogen autogen is not just a tool it's the future of collaborative AI where multiple Agents come together to transform ideas into reality and where AI agents unite innovate and Elevate what is different between autogen and Lang chain put simply autogen and Lang chain are both Frameworks for developing llm driven applications however there are some key differences between the two autogen is a multi-agent framework while Lang chain is a single agent framework autogen is more focused on code generation while Lang chain is more focused on\n",
            "\n",
            "is more focused on general purpose NLP tasks what is retrieval augmented generation retrieval augmented generation is an AI framework that retrieves data from external sources of knowledge to improve the quality of responses it ensures accuracy through techniques like vector similarity search and realtime updat to external data sets let's start coding we going start installing the necessary requirements assuming you have created a new python project and set up a virtual environment run the command then let's import the required dependencies we set up the list to autogen we create the config list as follows config list is a list containing configuration settings for the model you intend to use seed we set the seed to 42 for caching purposes with this configuration we are ready to use AI agents with autogen in this snippet we upload a PDF file and process it Pi pdf2 is used to read the PDF file text Splitter from L chain is used to split the text into chunks open AI embeddings is used for embedding the PDF file then f is used to create a vector store viice is used to convert the text chunks into Vector embeddings these vectors can then be used for various applications like similarity search Once the database has been created we can then query it we are going make use of the conversational retrieval chain from Lang chain to connect our similarity search to the prompts user input and the we call conversation buffer memory is a simple memory buffer that stores the history of the conversation Auden agents supports function calling for open AI models but we need to specify the function using the following snippet let's create an automated assistant agent named assistant with specific configurations the assistant is designed to interact with users and provide accurate responses this allows the assistant to read a PDF and generate llm an accurate answer user proxy agents are the types of AI agents that work on behalf of the user the implementation is typical but it includes\"\n",
            "\n",
            "\n",
            "CONCISE SUMMARY:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " This guide teaches how to use Langen autogen and function calls to build an AI chatbot. It is valuable for those new to these tools and interested in AI. The video shows the end result and how to use the code for personal projects. The guide also explains the differences between autogen and Lang chain, and introduces retrieval augmented generation, a framework for improving AI responses. The guide then goes into coding, including installing necessary requirements and setting up AI agents. It also covers creating a vector store and using conversational retrieval chain to connect similarity search to user input. Finally, it explains how to create an automated assistant agent and user proxy agents.\n"
          ]
        }
      ],
      "source": [
        "with get_openai_callback() as cb:\n",
        "    chain =load_summarize_chain(llm, chain_type=\"stuff\", verbose=True)\n",
        "    tx = chain.run(out[:2])\n",
        "    print(tx)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
